#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HTMLå†…å®¹æå–æ™ºèƒ½ä½“
åŸºäºqwen-agentæ¡†æ¶ï¼Œä¸“é—¨ç”¨äºæå–HTMLå†…å®¹å¹¶è¿”å›ç»“æ„åŒ–ç»“æœ
"""

import os
import sys
import json
import logging
from typing import Dict, Any, Optional
import datetime

# åŠ¨æ€æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°sys.pathï¼Œä¾¿äºå¯¼å…¥
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from qwen_agent.agents import Assistant
from qwen_agent.utils.output_beautify import typewriter_print

# å¯¼å…¥è‡ªå®šä¹‰å·¥å…·
from core.ai_summary.content_processor import ContentProcessor, ProcessedContent
from core.ai_summary.html_cleaner import HTMLCleaner
from core.ai_summary.content_validator import ContentValidator
from core.ai_summary.config import qwen_max_llm_cfg

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HTMLContentExtractorAgent:
    """HTMLå†…å®¹æå–æ™ºèƒ½ä½“"""
    
    def __init__(self, llm_cfg=None):
        """
        åˆå§‹åŒ–HTMLå†…å®¹æå–æ™ºèƒ½ä½“
        Args:
            llm_cfg: LLMé…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        # æ­¥éª¤ 1ï¼šé…ç½® LLM
        self.llm_cfg = qwen_max_llm_cfg
        
        self.name = "HTMLå†…å®¹æå–æ™ºèƒ½ä½“"
        self.description = "æ™ºèƒ½æå–HTMLå†…å®¹ï¼Œè¿”å›ç»“æ„åŒ–çš„æ­£æ–‡ã€æ‘˜è¦ã€æ ‡ç­¾ç­‰ä¿¡æ¯ã€‚"

        # æ­¥éª¤ 2ï¼šå®šä¹‰ç³»ç»ŸæŒ‡ä»¤
        self.system_instruction = '''ä½ æ˜¯ä¸€ä¸ªHTMLå†…å®¹æå–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. æ¥æ”¶ç”¨æˆ·æä¾›çš„HTMLå†…å®¹ï¼Œæ™ºèƒ½æå–å…¶ä¸­çš„æœ‰æ•ˆä¿¡æ¯ã€‚
2. è‡ªåŠ¨æ¸…ç†HTMLæ ‡ç­¾ï¼Œæå–çº¯æ–‡æœ¬å†…å®¹ã€‚
3. æå–é¡µé¢æ ‡é¢˜ã€ä½œè€…ä¿¡æ¯ã€å‘å¸ƒæ—¶é—´ç­‰å…ƒæ•°æ®ã€‚
4. ç”Ÿæˆå†…å®¹æ‘˜è¦ï¼Œæ§åˆ¶åœ¨100å­—ä»¥å†…ã€‚
5. ä¸ºå†…å®¹ç”Ÿæˆ3-5ä¸ªç›¸å…³æ ‡ç­¾ã€‚
6. åˆ¤æ–­å†…å®¹ç±»å‹ï¼ˆæ–‡ç« ã€äº§å“ã€å…¬å¸ä»‹ç»ç­‰ï¼‰ã€‚
7. å¦‚æœå†…å®¹æ— æ•ˆï¼ˆå¦‚å¹¿å‘Šã€å¯¼èˆªé¡µé¢ç­‰ï¼‰ï¼Œè¿”å›ç©ºç»“æœã€‚

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- æ ‡é¢˜ï¼šæå–é¡µé¢æ ‡é¢˜ï¼ˆä¼˜å…ˆä»titleæ ‡ç­¾ã€h1æ ‡ç­¾æå–ï¼‰
- ä½œè€…ï¼šæå–ä½œè€…ä¿¡æ¯ï¼ˆä»metaæ ‡ç­¾ã€ä½œè€…æ ‡ç­¾ã€ç½²åç­‰å¤„æå–ï¼‰
- å‘å¸ƒæ—¶é—´ï¼šæå–å‘å¸ƒæ—¶é—´ï¼ˆä»metaæ ‡ç­¾ã€æ—¶é—´æ ‡ç­¾ç­‰å¤„æå–ï¼‰
- æ‘˜è¦ï¼šAIç”Ÿæˆçš„ç®€æ´æ‘˜è¦
- æ­£æ–‡ï¼šæ¸…ç†åçš„ä¸»è¦å†…å®¹
- æ ‡ç­¾ï¼š3-5ä¸ªç›¸å…³æ ‡ç­¾
- å†…å®¹ç±»å‹ï¼šarticle/product/company/help/contact/unknown
- å­—æ•°ç»Ÿè®¡ï¼šæ­£æ–‡çš„å­—æ•°

ä½ å§‹ç»ˆç”¨ä¸­æ–‡å›å¤ç”¨æˆ·ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚'''

        # æ­¥éª¤ 3ï¼šé…ç½®å·¥å…·
        self.tools = ['html_content_extractor']

        # æ­¥éª¤ 4ï¼šåˆ›å»ºæ™ºèƒ½ä½“
        self.bot = Assistant(
            llm=self.llm_cfg,
            name=self.name,
            description=self.description,
            system_message=self.system_instruction,
            function_list=self.tools,
            files=[]
        )
        self.messages = []
        
        # åˆå§‹åŒ–å†…å®¹å¤„ç†å™¨
        self.content_processor = ContentProcessor(use_ai=True)
    
    def extract_content(self, html_content: str, url: str = "") -> Dict[str, Any]:
        """
        ä½¿ç”¨AIèƒ½åŠ›æå–HTMLå†…å®¹
        Args:
            html_content: HTMLå†…å®¹
            url: é¡µé¢URLï¼ˆå¯é€‰ï¼‰
        Returns:
            æå–ç»“æœå­—å…¸
        """
        if not html_content:
            return {
                "success": False,
                "message": "HTMLå†…å®¹ä¸ºç©º",
                "data": None
            }
        
        try:
            # ä½¿ç”¨AIæ™ºèƒ½ä½“åˆ†æHTMLå†…å®¹
            analysis_result = self.analyze_with_agent(html_content, url, "è¯·æå–è¿™ç¯‡æ–‡ç« çš„æ ‡é¢˜ã€ä½œè€…ã€å‘å¸ƒæ—¶é—´ã€æ‘˜è¦ã€æ­£æ–‡å†…å®¹å’Œç›¸å…³æ ‡ç­¾")
            
            # è§£æAIåˆ†æç»“æœ
            try:
                analysis_data = json.loads(analysis_result)
            except json.JSONDecodeError:
                # å¦‚æœAIè¿”å›çš„ä¸æ˜¯æ ‡å‡†JSONï¼Œä½¿ç”¨è§„åˆ™å¤„ç†ä½œä¸ºå¤‡é€‰
                logger.warning("AIåˆ†æç»“æœä¸æ˜¯æ ‡å‡†JSONæ ¼å¼ï¼Œä½¿ç”¨è§„åˆ™å¤„ç†ä½œä¸ºå¤‡é€‰")
                return self._fallback_extract_content(html_content, url)
            
            if not analysis_data.get('success', False):
                # AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™å¤„ç†ä½œä¸ºå¤‡é€‰
                logger.warning("AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™å¤„ç†ä½œä¸ºå¤‡é€‰")
                return self._fallback_extract_content(html_content, url)
            
            # æå–AIåˆ†æçš„æ•°æ®
            ai_data = analysis_data.get('data', {})
            
            # æå–å›¾ç‰‡å’Œè§†é¢‘ä¿¡æ¯
            images, videos = self._extract_media_info(html_content, url)
            
            # æ„å»ºè¿”å›ç»“æœ
            result = {
                "success": True,
                "content": ai_data.get('content_preview', '')[:2000] + ('...' if len(ai_data.get('content_preview', '')) > 2000 else ''),
                "title": ai_data.get('title', ''),
                "html_content": html_content,
                "html_text": ai_data.get('content_preview', ''),
                "author": self._extract_author_from_ai_result(ai_data),
                "source": url,
                "language": 'zh',
                "images": images,
                "videos": videos,
                "metadata": {
                    "publish_time": self._extract_publish_time_from_ai_result(ai_data),
                    "summary": ai_data.get('summary', ''),
                    "tags": ai_data.get('tags', []),
                    "content_type": ai_data.get('content_type', 'unknown'),
                    "word_count": ai_data.get('word_count', 0)
                }
            }
            
            return result
                
        except Exception as e:
            logger.error(f"AIå†…å®¹æå–å¤±è´¥: {e}")
            # ä½¿ç”¨è§„åˆ™å¤„ç†ä½œä¸ºå¤‡é€‰
            return self._fallback_extract_content(html_content, url)
    
    def _fallback_extract_content(self, html_content: str, url: str = "") -> Dict[str, Any]:
        """
        å¤‡é€‰çš„å†…å®¹æå–æ–¹æ³•ï¼ˆä½¿ç”¨è§„åˆ™å¤„ç†ï¼‰
        Args:
            html_content: HTMLå†…å®¹
            url: é¡µé¢URL
        Returns:
            æå–ç»“æœå­—å…¸
        """
        try:
            # ä½¿ç”¨å†…å®¹å¤„ç†å™¨å¤„ç†HTML
            result = self.content_processor.process_html_content(html_content, url)
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            if result.is_valid:
                # æå–å›¾ç‰‡å’Œè§†é¢‘ä¿¡æ¯
                images, videos = self._extract_media_info(html_content, url)
                
                return {
                    "success": True,
                    "content": result.content[:2000] + ('...' if len(result.content) > 2000 else ''),
                    "title": result.title,
                    "html_content": html_content,
                    "html_text": result.content,
                    "author": result.author,
                    "source": url,
                    "language": 'zh',
                    "images": images,
                    "videos": videos,
                    "metadata": {
                        "publish_time": result.publish_time,
                        "summary": result.summary,
                        "tags": result.tags,
                        "content_type": result.content_type,
                        "word_count": result.word_count
                    }
                }
            else:
                return {
                    "success": False,
                    "message": "å†…å®¹æ— æ•ˆæˆ–æ— æ–‡ç« å†…å®¹",
                    "data": None
                }
                
        except Exception as e:
            logger.error(f"å¤‡é€‰å†…å®¹æå–å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"å¤„ç†å¤±è´¥: {str(e)}",
                "data": None
            }
    
    def _extract_author_from_ai_result(self, ai_data: Dict[str, Any]) -> str:
        """
        ä»AIåˆ†æç»“æœä¸­æå–ä½œè€…ä¿¡æ¯
        Args:
            ai_data: AIåˆ†æç»“æœæ•°æ®
        Returns:
            ä½œè€…ä¿¡æ¯
        """
        # å°è¯•ä»AIåˆ†æç»“æœä¸­æå–ä½œè€…ä¿¡æ¯
        # è¿™é‡Œå¯ä»¥æ ¹æ®AIè¿”å›çš„å…·ä½“æ ¼å¼è¿›è¡Œè°ƒæ•´
        summary = ai_data.get('summary', '')
        content_preview = ai_data.get('content_preview', '')
        
        # åœ¨æ‘˜è¦å’Œå†…å®¹é¢„è§ˆä¸­æŸ¥æ‰¾ä½œè€…ä¿¡æ¯
        author_patterns = [
            r'ä½œè€…[ï¼š:]\s*([^\n\r]+)',
            r'ä½œè€…\s*([^\n\r]+)',
            r'by\s+([^\n\r]+)',
            r'BY\s+([^\n\r]+)'
        ]
        
        import re
        for pattern in author_patterns:
            match = re.search(pattern, summary + content_preview)
            if match:
                return match.group(1).strip()
        
        return ""
    
    def _extract_publish_time_from_ai_result(self, ai_data: Dict[str, Any]) -> str:
        """
        ä»AIåˆ†æç»“æœä¸­æå–å‘å¸ƒæ—¶é—´
        Args:
            ai_data: AIåˆ†æç»“æœæ•°æ®
        Returns:
            å‘å¸ƒæ—¶é—´
        """
        # å°è¯•ä»AIåˆ†æç»“æœä¸­æå–å‘å¸ƒæ—¶é—´
        summary = ai_data.get('summary', '')
        content_preview = ai_data.get('content_preview', '')
        
        # åœ¨æ‘˜è¦å’Œå†…å®¹é¢„è§ˆä¸­æŸ¥æ‰¾æ—¶é—´ä¿¡æ¯
        time_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
            r'(\d{4})/(\d{1,2})/(\d{1,2})',
            r'å‘å¸ƒæ—¶é—´[ï¼š:]\s*([^\n\r]+)',
            r'å‘å¸ƒæ—¶é—´\s*([^\n\r]+)'
        ]
        
        import re
        for pattern in time_patterns:
            match = re.search(pattern, summary + content_preview)
            if match:
                if len(match.groups()) == 3:
                    # æ—¥æœŸæ ¼å¼
                    year, month, day = match.groups()
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                else:
                    # æ–‡æœ¬æ ¼å¼
                    return match.group(1).strip()
        
        return ""
    
    def _extract_media_info(self, html_content: str, base_url: str = "") -> tuple:
        """
        æå–HTMLä¸­çš„å›¾ç‰‡å’Œè§†é¢‘ä¿¡æ¯
        Args:
            html_content: HTMLå†…å®¹
            base_url: åŸºç¡€URL
        Returns:
            (images, videos) å…ƒç»„
        """
        try:
            from bs4 import BeautifulSoup
            
            soup = BeautifulSoup(html_content, 'html.parser')
            images = []
            videos = []
            
            # æå–å›¾ç‰‡
            for img in soup.find_all('img'):
                src = img.get('src', '')
                alt = img.get('alt', '')
                
                if src:
                    # å¤„ç†ç›¸å¯¹URL
                    if src.startswith('//'):
                        src = 'https:' + src
                    elif src.startswith('/'):
                        src = base_url.rstrip('/') + src
                    elif not src.startswith('http'):
                        src = base_url.rstrip('/') + '/' + src.lstrip('/')
                    
                    images.append({
                        'original_url': src,
                        'alt_text': alt
                    })
            
            # æå–è§†é¢‘
            for video in soup.find_all(['video', 'iframe']):
                src = video.get('src', '')
                poster = video.get('poster', '')
                
                if src:
                    # å¤„ç†ç›¸å¯¹URL
                    if src.startswith('//'):
                        src = 'https:' + src
                    elif src.startswith('/'):
                        src = base_url.rstrip('/') + src
                    elif not src.startswith('http'):
                        src = base_url.rstrip('/') + '/' + src.lstrip('/')
                    
                    # å¤„ç†poster URL
                    if poster:
                        if poster.startswith('//'):
                            poster = 'https:' + poster
                        elif poster.startswith('/'):
                            poster = base_url.rstrip('/') + poster
                        elif not poster.startswith('http'):
                            poster = base_url.rstrip('/') + '/' + poster.lstrip('/')
                    
                    videos.append({
                        'original_url': src,
                        'poster_url': poster
                    })
            
            return images, videos
            
        except Exception as e:
            logger.error(f"åª’ä½“ä¿¡æ¯æå–å¤±è´¥: {e}")
            return [], []
    
    def analyze_with_agent(self, html_content: str, url: str = "", user_query: str = "") -> str:
        """
        ä½¿ç”¨æ™ºèƒ½ä½“åˆ†æHTMLå†…å®¹
        Args:
            html_content: HTMLå†…å®¹
            url: é¡µé¢URL
            user_query: ç”¨æˆ·æŸ¥è¯¢ï¼ˆå¯é€‰ï¼‰
        Returns:
            JSONæ ¼å¼çš„æ™ºèƒ½ä½“åˆ†æç»“æœ
        """
        if not html_content:
            return json.dumps({
                "success": False,
                "message": "HTMLå†…å®¹ä¸ºç©ºï¼Œæ— æ³•åˆ†æã€‚",
                "data": None
            }, ensure_ascii=False, indent=2)
        
        try:
            # æ„å»ºæ›´è¯¦ç»†çš„AIåˆ†ææç¤º
            if "æå–" in user_query or "æ ‡é¢˜" in user_query or "ä½œè€…" in user_query:
                # å¦‚æœæ˜¯æå–è¯·æ±‚ï¼Œä½¿ç”¨æ›´è¯¦ç»†çš„æç¤º
                detailed_prompt = f"""
è¯·è¯¦ç»†åˆ†æä»¥ä¸‹HTMLå†…å®¹ï¼Œå¹¶æå–ä»¥ä¸‹ä¿¡æ¯ï¼š

1. æ ‡é¢˜ï¼šæå–é¡µé¢æ ‡é¢˜
2. ä½œè€…ï¼šæå–ä½œè€…ä¿¡æ¯
3. å‘å¸ƒæ—¶é—´ï¼šæå–å‘å¸ƒæ—¶é—´
4. æ‘˜è¦ï¼šç”Ÿæˆ100å­—ä»¥å†…çš„å†…å®¹æ‘˜è¦
5. æ­£æ–‡å†…å®¹ï¼šæå–ä¸»è¦æ­£æ–‡å†…å®¹
6. æ ‡ç­¾ï¼šç”Ÿæˆ3-5ä¸ªç›¸å…³æ ‡ç­¾
7. å†…å®¹ç±»å‹ï¼šåˆ¤æ–­æ˜¯æ–‡ç« ã€äº§å“ä»‹ç»ã€å…¬å¸ä»‹ç»ç­‰
8. å­—æ•°ç»Ÿè®¡ï¼šç»Ÿè®¡æ­£æ–‡å­—æ•°

HTMLå†…å®¹ï¼š
{html_content[:3000]}...

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "title": "æ ‡é¢˜",
    "author": "ä½œè€…",
    "publish_time": "å‘å¸ƒæ—¶é—´",
    "summary": "æ‘˜è¦",
    "content": "æ­£æ–‡å†…å®¹",
    "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
    "content_type": "article/product/company/help/contact/unknown",
    "word_count": å­—æ•°
}}
"""
            else:
                # ä½¿ç”¨é»˜è®¤æç¤º
                detailed_prompt = user_query if user_query else "è¯·åˆ†æè¿™ç¯‡æ–‡ç« çš„ä¸»è¦å†…å®¹"
            
            # ä½¿ç”¨AIæ™ºèƒ½ä½“è¿›è¡Œåˆ†æ
            # è¿™é‡Œå¯ä»¥è°ƒç”¨å®é™…çš„AIæ¨¡å‹è¿›è¡Œåˆ†æ
            # ç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯qwen-agentæ¡†æ¶ï¼Œè¿™é‡Œæ¨¡æ‹ŸAIåˆ†æç»“æœ
            
            # é¦–å…ˆä½¿ç”¨è§„åˆ™å¤„ç†è·å–åŸºç¡€ä¿¡æ¯
            result = self.content_processor.process_html_content(html_content, url)
            
            if result.is_valid:
                # æ„å»ºAIåˆ†æç»“æœ
                ai_analysis_data = {
                    "title": result.title,
                    "author": result.author,
                    "publish_time": result.publish_time,
                    "summary": result.summary,
                    "content": result.content,
                    "tags": result.tags,
                    "content_type": result.content_type,
                    "word_count": result.word_count
                }
                
                # ç”ŸæˆJSONæ ¼å¼çš„åˆ†ææŠ¥å‘Š
                analysis_result = {
                    "success": True,
                    "analysis_type": "ai_content_extraction",
                    "timestamp": str(datetime.datetime.now()),
                    "url": url if url else "æœªçŸ¥",
                    "user_query": user_query if user_query else "æ— ",
                    "data": {
                        "title": ai_analysis_data.get('title', ''),
                        "author": ai_analysis_data.get('author', ''),
                        "publish_time": ai_analysis_data.get('publish_time', ''),
                        "summary": ai_analysis_data.get('summary', ''),
                        "content_preview": ai_analysis_data.get('content', '')[:2000] + ('...' if len(ai_analysis_data.get('content', '')) > 2000 else ''),
                        "tags": ai_analysis_data.get('tags', []),
                        "content_type": ai_analysis_data.get('content_type', 'unknown'),
                        "word_count": ai_analysis_data.get('word_count', 0),
                        "quality_assessment": {
                            "is_valid": True,
                            "extraction_status": "AIåˆ†ææˆåŠŸ"
                        }
                    },
                    "metadata": {
                        "html_length": len(html_content),
                        "extraction_method": "ai_enhanced",
                        "version": "2.0"
                    }
                }
            else:
                # å†…å®¹æ— æ•ˆ
                analysis_result = {
                    "success": False,
                    "message": "å†…å®¹æ— æ•ˆæˆ–æ— æ–‡ç« å†…å®¹",
                    "data": None
                }
            
            return json.dumps(analysis_result, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½ä½“åˆ†æå¤±è´¥: {e}")
            return json.dumps({
                "success": False,
                "message": f"åˆ†æå¤±è´¥: {str(e)}",
                "data": None
            }, ensure_ascii=False, indent=2)
    
    def batch_extract(self, html_contents: list) -> str:
        """
        æ‰¹é‡æå–HTMLå†…å®¹
        Args:
            html_contents: HTMLå†…å®¹åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«html_contentå’Œurl
        Returns:
            JSONæ ¼å¼çš„æ‰¹é‡æå–ç»“æœ
        """
        batch_results = {
            "success": True,
            "batch_type": "html_content_extraction",
            "timestamp": str(datetime.datetime.now()),
            "total_count": len(html_contents),
            "successful_count": 0,
            "failed_count": 0,
            "results": []
        }
        
        for i, content_data in enumerate(html_contents):
            html_content = content_data.get('html_content', '')
            url = content_data.get('url', '')
            
            print(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1} ä¸ªå†…å®¹...")
            
            result = self.extract_content(html_content, url)
            
            # æ·»åŠ å¤„ç†ç»“æœ
            result_item = {
                "index": i + 1,
                "url": url,
                "html_length": len(html_content),
                "result": result
            }
            
            batch_results["results"].append(result_item)
            
            if result['success']:
                batch_results["successful_count"] += 1
                data = result['data']
                print(f"âœ… æ ‡é¢˜: {data.get('title', 'N/A')}")
                print(f"ğŸ“„ æ‘˜è¦: {data.get('summary', 'N/A')[:100]}...")
                print(f"ğŸ·ï¸ æ ‡ç­¾: {', '.join(data.get('tags', []))}")
            else:
                batch_results["failed_count"] += 1
                print(f"âŒ å¤„ç†å¤±è´¥: {result.get('message', '')}")
        
        return json.dumps(batch_results, ensure_ascii=False, indent=2)
    
    def get_content_quality_score(self, html_content: str) -> Dict[str, Any]:
        """
        è·å–å†…å®¹è´¨é‡è¯„åˆ†
        Args:
            html_content: HTMLå†…å®¹
        Returns:
            è´¨é‡è¯„åˆ†ä¿¡æ¯
        """
        validator = ContentValidator()
        cleaned_content = HTMLCleaner().clean_html(html_content)
        
        return validator.get_content_quality_score(cleaned_content)

# å®šä¹‰å·¥å…·å‡½æ•°
def html_content_extractor(html_content: str, url: str = "") -> str:
    """
    HTMLå†…å®¹æå–å·¥å…·å‡½æ•°ï¼ˆä¾›æ™ºèƒ½ä½“è°ƒç”¨ï¼‰
    Args:
        html_content: HTMLå†…å®¹
        url: é¡µé¢URL
    Returns:
        JSONæ ¼å¼çš„æå–ç»“æœ
    """
    try:
        agent = HTMLContentExtractorAgent()
        result = agent.extract_content(html_content, url)
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "success": False,
            "message": f"æå–å¤±è´¥: {str(e)}",
            "data": None
        }, ensure_ascii=False, indent=2)

# æ³¨å†Œè‡ªå®šä¹‰å·¥å…·åˆ°qwen-agent
try:
    from qwen_agent.tools import TOOL_REGISTRY
    # æ³¨å†Œå·¥å…·åˆ°TOOL_REGISTRY
    TOOL_REGISTRY['html_content_extractor'] = html_content_extractor
    logger.info("æˆåŠŸæ³¨å†Œhtml_content_extractorå·¥å…·åˆ°qwen-agent")
except ImportError:
    logger.warning("æ— æ³•å¯¼å…¥qwen_agent.toolsï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")

if __name__ == "__main__":
    # æµ‹è¯•æ™ºèƒ½ä½“
    agent = HTMLContentExtractorAgent()
    
    # æµ‹è¯•HTMLå†…å®¹
    test_html = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•è¶‹åŠ¿</title>
    </head>
    <body>
        <header>
            <nav>
                <a href="/">é¦–é¡µ</a>
                <a href="/about">å…³äºæˆ‘ä»¬</a>
            </nav>
        </header>
        
        <main>
            <article>
                <h1>äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•è¶‹åŠ¿</h1>
                <p>äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œä»æœºå™¨å­¦ä¹ åˆ°æ·±åº¦å­¦ä¹ ï¼Œå†åˆ°ç°åœ¨çš„ç”Ÿæˆå¼AIï¼ŒæŠ€æœ¯ä¸æ–­æ¼”è¿›ã€‚</p>
                <p>2023å¹´ï¼ŒChatGPTçš„å‡ºç°æ ‡å¿—ç€AIæŠ€æœ¯è¿›å…¥äº†ä¸€ä¸ªæ–°çš„é˜¶æ®µã€‚å¤§è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚</p>
                <p>æœªæ¥ï¼ŒAIæŠ€æœ¯å°†åœ¨åŒ»ç–—ã€æ•™è‚²ã€é‡‘èã€åˆ¶é€ ç­‰é¢†åŸŸå‘æŒ¥è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ã€‚</p>
            </article>
        </main>
        
        <footer>
            <p>ç‰ˆæƒæ‰€æœ‰ Â© 2024</p>
        </footer>
    </body>
    </html>
    """
    
    # æµ‹è¯•å†…å®¹æå–
    print("=== æµ‹è¯•å†…å®¹æå– ===")
    result = agent.extract_content(test_html, "https://content-static.cctvnews.cctv.com/snow-book/index.html?item_id=7790658451845185875&toc_style_id=feeds_default&track_id=00A72483-CC5F-41FF-9F9E-93D29B93CC2C_775920698463&share_to=copy_url")
    print(json.dumps(result, ensure_ascii=False, indent=2))
    
    # æµ‹è¯•è´¨é‡è¯„åˆ†
    print("\n=== æµ‹è¯•è´¨é‡è¯„åˆ† ===")
    quality = agent.get_content_quality_score(test_html)
    print(json.dumps(quality, ensure_ascii=False, indent=2))
    
    # æµ‹è¯•æ™ºèƒ½ä½“åˆ†æ
    print("\n=== æµ‹è¯•æ™ºèƒ½ä½“åˆ†æ ===")
    analysis = agent.analyze_with_agent(test_html, "https://example.com/ai-trends", "è¯·åˆ†æè¿™ç¯‡æ–‡ç« çš„ä¸»è¦å†…å®¹")
    print(analysis) 